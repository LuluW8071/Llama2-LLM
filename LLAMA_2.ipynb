{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3//o8hYOzPS2YuQVh+8RB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92f4a4a667fe43a09163ffd65c315436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69c9826d17e9411a8a73e1101a96bf6f",
              "IPY_MODEL_81beedb2cf364d2f9bff3159cb3dd3ef",
              "IPY_MODEL_2602d11167514f55af8601c554d5de78"
            ],
            "layout": "IPY_MODEL_790abf4115634f9db3bb68738ae72085"
          }
        },
        "69c9826d17e9411a8a73e1101a96bf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_144b0dc810354255b5fc47363510f1c9",
            "placeholder": "​",
            "style": "IPY_MODEL_404142166552487d9dcba057f5fa48b0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81beedb2cf364d2f9bff3159cb3dd3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63401d2ba16d4d6e8d8acbeb79c62e8e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f48e33f44a9149abafdd18d795294692",
            "value": 2
          }
        },
        "2602d11167514f55af8601c554d5de78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a966338d354ad88a635032ece08946",
            "placeholder": "​",
            "style": "IPY_MODEL_2d1ff2c7b3df45d79e031f533340d4c2",
            "value": " 2/2 [01:06&lt;00:00, 30.14s/it]"
          }
        },
        "790abf4115634f9db3bb68738ae72085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144b0dc810354255b5fc47363510f1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404142166552487d9dcba057f5fa48b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63401d2ba16d4d6e8d8acbeb79c62e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48e33f44a9149abafdd18d795294692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7a966338d354ad88a635032ece08946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1ff2c7b3df45d79e031f533340d4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuluW8071/Llama2-LLM-Text-Generation/blob/main/LLAMA_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Llama 2](https://llama.meta.com/llama2) and Model Fine-Tuning\n",
        "\n",
        "**Llama 2** is a collection of second-generation open-source Large Language Models (LLMs) from Meta, designed to handle a wide range of natural language processing tasks. These models range in scale from `7 billion to 70 billion parameters`.\n",
        "\n",
        "**Llama-2-Chat**, optimized for dialogue, has shown similar performance to popular closed-source models like ChatGPT and PaLM.\n",
        "\n",
        "**Fine-tuning** in machine learning involves adjusting the weights and parameters of a pre-trained model on new data to improve its performance on a specific task. It includes training the model on a new dataset specific to the task at hand, while updating the model's weights to adapt to the new data.\n",
        "\n",
        "<img src = \"https://images.datacamp.com/image/upload/v1697724450/Fine_Tune_L_La_MA_2_cc6aa0e4ad.png\">"
      ],
      "metadata": {
        "id": "DY6aVep7dYqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZUd-6ZvQQ0s",
        "outputId": "77feee63-bbad-40a6-919c-8e02054d1344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install accelerate peft bitsandbytes transformers trl\n",
        "!pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Qmut6xQlPb",
        "outputId": "2ae901fa-3adb-4a64-985e-fa58237b256f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.10)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.8.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.1.0+cu121)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.7.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.15)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.6.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "x9wLP-MScrcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "# Setting up device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8buO2ujZQrGB",
        "outputId": "7c114395-87c7-44d5-bd11-aa3ca1a2714d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "Using NousResearch’s `Llama-2-7b-chat-hf` as our base model. It is the same as the original Meta’s official `Llama-2 model` from Hugging Face but easily accessible.\n",
        "\n",
        "### [See Guanaco Dataset](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)"
      ],
      "metadata": {
        "id": "xdxwxusrg0wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model from Hugging Face hub with 7 billion parameters\n",
        "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# New instruction dataset\n",
        "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model\n",
        "new_model = \"llama-2-7b-chat-guanaco\""
      ],
      "metadata": {
        "id": "wrU7OrhmREgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset, model, and tokenizer"
      ],
      "metadata": {
        "id": "GcWSsllYhxIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(guanaco_dataset, split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SP5MjlMROWu",
        "outputId": "25e77cc8-87df-42ea-d092-847bf373777f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-bit Quantization Configuration\n",
        "4-bit Quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for real-world applications.\n",
        "\n",
        "QLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model.\n",
        "\n",
        "During fine-tuning, gradients are backpropagated through the frozen 4-bit quantized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at 4 bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance.\n",
        "\n",
        "<img src = \"https://images.datacamp.com/image/upload/v1697713094/image7_3e12912d0d.png\">\n",
        "\n",
        "### [Paper on QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)"
      ],
      "metadata": {
        "id": "HY4OZODqRdu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "TaKsZZuURSB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading `Llama 2 model`"
      ],
      "metadata": {
        "id": "1sggUH8uiPiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "92f4a4a667fe43a09163ffd65c315436",
            "69c9826d17e9411a8a73e1101a96bf6f",
            "81beedb2cf364d2f9bff3159cb3dd3ef",
            "2602d11167514f55af8601c554d5de78",
            "790abf4115634f9db3bb68738ae72085",
            "144b0dc810354255b5fc47363510f1c9",
            "404142166552487d9dcba057f5fa48b0",
            "63401d2ba16d4d6e8d8acbeb79c62e8e",
            "f48e33f44a9149abafdd18d795294692",
            "b7a966338d354ad88a635032ece08946",
            "2d1ff2c7b3df45d79e031f533340d4c2"
          ]
        },
        "id": "xgtdGgPER54I",
        "outputId": "a51d3d87-e765-4cd3-bcd8-c35045fefcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92f4a4a667fe43a09163ffd65c315436"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Tokenizers"
      ],
      "metadata": {
        "id": "CCIlQXknUDM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "hFdzA4xyR8xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT Parameters\n",
        "Traditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data.\n",
        "\n",
        "Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's parameters, making it much more efficient. Learn about parameters by reading the [PEFT official documentation](https://huggingface.co/docs/peft/conceptual_guides/lora)."
      ],
      "metadata": {
        "id": "e__vMu1TUM80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.1,\n",
        "    r = 64,\n",
        "    bias = \"none\",\n",
        "    task_type = \"CAUSAL_LM\",)"
      ],
      "metadata": {
        "id": "sqTJQ-sOUIDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters\n",
        "\n",
        "| Hyperparameter               | Description                                       |\n",
        "|------------------------------|---------------------------------------------------|\n",
        "| output_dir                   | Output directory for storing model predictions and checkpoints.             |\n",
        "| num_train_epochs             | Number of training epochs.                        |\n",
        "| fp16/bf16                    | Disable fp16/bf16 training.                       |\n",
        "| per_device_train_batch_size  | Batch size per GPU for training.                  |\n",
        "| per_device_eval_batch_size   | Batch size per GPU for evaluation.                |\n",
        "| gradient_accumulation_steps  | Number of steps required to accumulate gradients during update.             |\n",
        "| gradient_checkpointing       | Enable gradient checkpointing.                    |\n",
        "| max_grad_norm                | Gradient clipping.                                |\n",
        "| learning_rate                | Initial learning rate.                            |\n",
        "| weight_decay                 | Weight decay applied to all layers except bias/LayerNorm weights.            |\n",
        "| Optim                        | Model optimizer (AdamW optimizer).                 |\n",
        "| lr_scheduler_type            | Learning rate schedule.                           |\n",
        "| max_steps                    | Number of training steps.                         |\n",
        "| warmup_ratio                 | Ratio of steps for linear warmup.                 |\n",
        "| group_by_length              | Improve performance and accelerate training.      |\n",
        "| save_steps                   | Save checkpoint every 25 update steps.            |\n",
        "| logging_steps                | Log every 25 update steps.                        |\n"
      ],
      "metadata": {
        "id": "7DEJ3hkCU0EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 1,              # Start with 1 epoch and increase gradually if memory allows\n",
        "    per_device_train_batch_size = 2,   # Begin with smallest batch size, increase in increments of 1\n",
        "    gradient_accumulation_steps = 8,   # Aggressively accumulate gradients to compensate for low batch size\n",
        "    optim = \"adamw_torch\",             # Efficient optimizer for LLMs\n",
        "    save_steps = 1000,                 # Adjust saving frequency based on training duration\n",
        "    logging_steps = 1000,              # Adjust logging frequency based on your preference\n",
        "    learning_rate = 5e-6,              # Start with very low learning rate to mitigate instability\n",
        "    weight_decay = 0.01,               # Regularization to prevent overfitting\n",
        "    fp16 = True,                       # Enable mixed precision for memory savings\n",
        "    bf16 = False,                      # T4 doesn't support bfloat16\n",
        "    max_grad_norm = 0.5,               # Adjust gradient norm as needed\n",
        "    max_steps = -1,                    # Train for all epochs by default\n",
        "    warmup_ratio = 0.1,                # Adjust warmup ratio based on learning rate and dataset size\n",
        "    group_by_length = True,            # Improve efficiency for long sequences\n",
        "    lr_scheduler_type = \"constant\",    # Use warmup followed by constant learning rate\n",
        "    report_to = \"tensorboard\",         # Track training progress with TensorBoard\n",
        "\n",
        "    # Additional memory-specific optimizations:\n",
        "    # max_train_steps = 1000,          # Set a maximum number of training steps to limit total memory usage\n",
        "    # sharded_ddp = True,              # Enable DistributedDataParallel sharding if multiple GPUs are available\n",
        "    gradient_checkpointing = True,     # Recompute intermediate activations for memory savings\n",
        "    fp16_full_eval = True,             # Use mixed precision during evaluation as well\n",
        "    dataloader_pin_memory = False,     # Disable data pinning to avoid potential memory overhead\n",
        "    local_rank = -1,                   # Disable automatic distributed training (if only 1 GPU)\n",
        "    # skip_memory_check=True,          # Temporarily skip memory checks, but monitor closely\n",
        ")"
      ],
      "metadata": {
        "id": "0-QV5OPMUIhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fine-tuning\n",
        "Supervised fine-tuning (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally proximal policy optimization (PPO).\n",
        "\n",
        "Provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters."
      ],
      "metadata": {
        "id": "tw7esRq1ixoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    peft_config = peft_params,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = None,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_params,\n",
        "    packing = False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MST-DxCfUjLS",
        "outputId": "b4e68420-6014-4622-e75a-6251e172b850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(new_model)\n",
        "trainer.tokenizer.save_pretrained(new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P8900RDU5w6",
        "outputId": "55165271-a879-40d8-8e70-d7f5d4ca1641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('llama-2-7b-chat-guanaco/tokenizer_config.json',\n",
              " 'llama-2-7b-chat-guanaco/special_tokens_map.json',\n",
              " 'llama-2-7b-chat-guanaco/tokenizer.model',\n",
              " 'llama-2-7b-chat-guanaco/added_tokens.json',\n",
              " 'llama-2-7b-chat-guanaco/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorboard import notebook\n",
        "# log_dir = \"results/runs\"\n",
        "# notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
      ],
      "metadata": {
        "id": "o-kpGvRcV04w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Text Generation"
      ],
      "metadata": {
        "id": "VL4U2SEBi8fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"task\": \"text-generation\",\n",
        "    \"model\": model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"max_length\": 250,\n",
        "    \"config\": {\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ok_GunzWr3tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "prompt = \"Who is Leonardo Da Vinci?\"\n",
        "pipe = pipeline(**config)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3526yjpoV-Ok",
        "outputId": "fdcbd630-3080-44c4-e533-1823f919b848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Who is Leonardo Da Vinci? [/INST]  Leonardo da Vinci (1452-1519) was a true Renaissance man, a polymath who excelled in various fields, including art, science, engineering, mathematics, and anatomy. everybody knows him as the most famous artist of the Italian Renaissance, but he was also a prolific inventor, engineer, and scientist. Here are some key facts about Leonardo da Vinci:\n",
            "\n",
            "1. Early Life: Leonardo was born in Vinci, Italy, on April 15, 1452. His father, Messer Piero Fruosini, was a notary, and his mother, Caterina Buti, was a peasant.\n",
            "2. Artistic Career: Leonardo began his artistic career as a young man in Florence, where he was apprenticed to the artist Andrea del Verrocchio. He became one of the most renowned painters of his time, creating masterpieces such as the Mona Lisa, The Last Supper, and Virgin of the Rocks.\n",
            "3. Inventions and Engineering: Leon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is github?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(f\"{prompt}\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB1arJ2pWPr8",
        "outputId": "33cdbc41-47b3-4ce9-8376-8cc84f1bd623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is github?\n",
            " nobody knows.\n",
            "\n",
            "But seriously, GitHub is a web-based platform that allows developers to store, manage, and collaborate on code projects. It was founded in 2008 by Chris Wanstrath, Scott Chacon, and Tom Prestley, and has since become one of the most popular platforms for software development and version control.\n",
            "\n",
            "Here are some key features of GitHub:\n",
            "\n",
            "1. Version control: GitHub allows developers to store and manage different versions of their code, making it easier to track changes and collaborate with others.\n",
            "2. Collaboration: GitHub enables developers to invite others to collaborate on a project, allowing multiple people to work on the same codebase simultaneously.\n",
            "3. Code reviews: GitHub provides a feature called \"code reviews\" that allows developers to review each other's code changes, ensuring that the code is high-quality and meets the project's requirements.\n",
            "4. Issue tracking: GitHub allows developers to track issues and bugs in the code, making it easier to identify and fix problems.\n",
            "5. Integration with other tools: GitHub can be integrated with other tools and services, such as continuous integration (CI) and continuous deployment (CD) tools, project management tools, and more.\n",
            "\n",
            "Overall, GitHub is a powerful platform that has revolutionized the way developers work together and manage their code projects. It has become an essential tool for many developers and organizations around the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is youtube?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
        "result = pipe(f\"{prompt}\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXLyvtYkkJS",
        "outputId": "48daf438-b810-4518-dad2-be8793d40573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is youtube?\n",
            " nobody knows.\n",
            "\n",
            "Answer: YouTube is a video-sharing platform where users can upload, share, and view videos. It was founded in 2005 by Steve Chen, Chad Hurley, and Jawed Karim and was later acquired by Google in 2006. YouTube has become one of the most popular websites on the internet, with billions of users and millions of hours of content available to watch. Users can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Are you dumb?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(f\"{prompt}\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP1TowbhkoRg",
        "outputId": "8f83f314-1535-40ac-e812-68c47ad9250b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are you dumb?\n",
            " Unterscheidung between the two is not always clear-cut, and different people may have different opinions on the matter. However, here are some general differences between the two:\n",
            "\n",
            "1. **Definition:** **Dumb** generally refers to something that is stupid or foolish, while **dumb** can refer to a person who is lacking in intelligence or mental ability.\n",
            "\n",
            "2. **Usage:** **Dumb** is often used in a derogatory manner to insult or belittle someone, while **dumb** is sometimes used in a more neutral or even affectionate way to describe someone who is not very intelligent or capable.\n",
            "\n",
            "3. **Pronunciation:** **Dumb** is pronounced with a long \"u\" sound (like \"pool\"), while **dumb** is pronounced with a short \"u\" sound (like \"putt\").\n",
            "\n",
            "4. **Etymology:** **Dumb** comes from the Old English word \"dumbe,\" which means \"dull or stupid,\" while **dumb** comes from the Latin word \"dumbus,\" which means \"mute.\"\n",
            "\n",
            "5. **Examples:** **Dumb** might be used in a sentence like \"That was a really dumb thing to do,\" while **dumb** might be used in a sentence like \"She's a bit dumb when it comes to math.\"\n",
            "\n",
            "It's worth noting that the distinction between these two words is not always observed in colloquial language, and some people may use them interchangeably or with different connotations. However, in general, **dumb** is used to describe a lack of intelligence or mental ability, while **dumb** is used to describe something that is stupid or foolish.\n"
          ]
        }
      ]
    }
  ]
}