# Llama2-LLM-Text-Generation <img src="https://user-images.githubusercontent.com/74038190/213844263-a8897a51-32f4-4b3b-b5c2-e1528b89f6f3.png" width="40px" />

[**Llama 2**](https://llama.meta.com/llama2) is a collection of second-generation open-source Large Language Models (LLMs) from Meta, designed to handle a wide range of natural language processing tasks. These models range in scale from `7 billion to 70 billion parameters`.
It is optimized for dialogue, has shown similar performance to popular closed-source models like **ChatGPT** and **PaLM**.

The repository implementation contains `Text Generation on Pre-Trained NousResearchâ€™s Llama-2-7b-chat-hf using [guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k) dataset`

<img src = "https://images.datacamp.com/image/upload/v1697724450/Fine_Tune_L_La_MA_2_cc6aa0e4ad.png" >


---
### Note: </br>
This repo just contains my understanding on `Llama2` LLM and trying to fine tune for faster text generation with minimal consumption of GPU compute resource. </br>
_Feel free to send issues if you face any problem._ 
